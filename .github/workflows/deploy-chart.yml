name: Helm Chart Deployment (Dev + Prod) â€” Zero Downtime

on:
  workflow_dispatch:
    inputs:
      imageRepo:
        description: "Docker image repository (ECR)"
        required: false
        default: "990060748279.dkr.ecr.us-east-1.amazonaws.com/swapp-dev"
      imageTag:
        description: "Docker image tag"
        required: false
        default: "latest"

  push:
    branches:
      - dev
      - main

# ðŸš¦ Prevent multiple Helm jobs from colliding
concurrency:
  group: helm-deploy
  cancel-in-progress: false

env:
  K8S_SERVER: https://8E8E0B619BB89BD85A4298A1A6C979C3.gr7.ap-south-1.eks.amazonaws.com
  K8S_TOKEN: ${{ secrets.K8S_TOKEN }}

jobs:
  deploy:
    runs-on: ubuntu-22.04

    strategy:
      matrix:
        include:
          - branch: dev
            release: webapp-dev
            namespace: dev
            valuesFile: charts/webapp/values/dev.yaml
          - branch: main
            release: webapp-prod
            namespace: production
            valuesFile: charts/webapp/values/prod.yaml

    steps:
      - name: Checkout Repository
        if: github.ref == format('refs/heads/{0}', matrix.branch)
        uses: actions/checkout@v4

      - name: Set up Helm
        if: github.ref == format('refs/heads/{0}', matrix.branch)
        uses: azure/setup-helm@v3
        with:
          version: "3.13.0"

      - name: Set up Kubectl
        if: github.ref == format('refs/heads/{0}', matrix.branch)
        uses: azure/setup-kubectl@v3
        with:
          version: "latest"

      - name: Configure kubeconfig
        if: github.ref == format('refs/heads/{0}', matrix.branch)
        run: |
          mkdir -p ~/.kube
          cat <<EOF > ~/.kube/config
          apiVersion: v1
          kind: Config
          clusters:
          - cluster:
              insecure-skip-tls-verify: true
              server: ${K8S_SERVER}
            name: eks-cluster
          contexts:
          - context:
              cluster: eks-cluster
              user: sa-user
            name: eks-context
          current-context: eks-context
          users:
          - name: sa-user
            user:
              token: ${K8S_TOKEN}
          EOF

      - name: Verify Cluster Access
        if: github.ref == format('refs/heads/{0}', matrix.branch)
        run: |
          echo "Checking cluster connectivity..."
          kubectl cluster-info
          kubectl get nodes -o wide

      - name: Ensure Namespace Exists
        if: github.ref == format('refs/heads/{0}', matrix.branch)
        run: |
          kubectl create namespace ${{ matrix.namespace }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Clear Helm Pending State if Any
        if: github.ref == format('refs/heads/{0}', matrix.branch)
        run: |
          set -e
          RELEASE=${{ matrix.release }}
          NS=${{ matrix.namespace }}
          echo "Checking Helm release $RELEASE in namespace $NS..."
          if helm history $RELEASE -n $NS | grep -E 'pending-upgrade|pending-rollback'; then
            echo "Release $RELEASE is stuck in pending state, rolling back..."
            LAST_GOOD=$(helm history $RELEASE -n $NS --output json | jq '[.[] | select(.status == "deployed")][-1].revision')
            if [ -n "$LAST_GOOD" ]; then
              helm rollback $RELEASE $LAST_GOOD -n $NS
            else
              echo "No good revision found, cleaning Helm secrets..."
              kubectl delete secret -n $NS -l owner=helm,name=$RELEASE || true
            fi
          else
            echo "No pending state detected, safe to continue."
          fi

      - name: Adopt Existing Resources
        if: github.ref == format('refs/heads/{0}', matrix.branch)
        run: |
          RELEASE=${{ matrix.release }}
          NS=${{ matrix.namespace }}
          for TYPE in deployment svc ingress configmap; do
            for NAME in $(kubectl get $TYPE -n $NS --no-headers -o custom-columns=":metadata.name" | grep $RELEASE || true); do
              echo "Adopting $TYPE $NAME ..."
              kubectl -n $NS annotate $TYPE $NAME meta.helm.sh/release-name=$RELEASE meta.helm.sh/release-namespace=$NS --overwrite
              kubectl -n $NS label $TYPE $NAME app.kubernetes.io/managed-by=Helm --overwrite
            done
          done

      - name: Deploy Helm Chart
        if: github.ref == format('refs/heads/{0}', matrix.branch)
        working-directory: ./helm-gitops-project
        run: |
          echo "Deploying ${{ matrix.release }} to ${{ matrix.namespace }} namespace..."
          helm upgrade --install ${{ matrix.release }} ./charts/webapp \
            --values ${{ matrix.valuesFile }} \
            --set image.repository=${{ github.event.inputs.imageRepo }} \
            --set image.tag=${{ github.event.inputs.imageTag }} \
            -n ${{ matrix.namespace }} --atomic --cleanup-on-fail --timeout 5m --wait

      - name: Verify Deployment
        if: github.ref == format('refs/heads/{0}', matrix.branch)
        run: |
          echo "Checking resources in ${{ matrix.namespace }} namespace..."
          kubectl get all -n ${{ matrix.namespace }} | grep ${{ matrix.release }} || true
          kubectl rollout status deployment/${{ matrix.release }} -n ${{ matrix.namespace }} --timeout=180s
          kubectl get ingress -n ${{ matrix.namespace }} | grep ${{ matrix.release }} || true

      - name: Debug Pods if Rollout Fails
        if: failure() && github.ref == format('refs/heads/{0}', matrix.branch)
        run: |
          echo "DEBUG: Showing pod details in ${{ matrix.namespace }} namespace"
          kubectl get pods -n ${{ matrix.namespace }} -o wide
          kubectl describe pods -n ${{ matrix.namespace }} | tail -n 100
          POD=$(kubectl get pods -n ${{ matrix.namespace }} -l app=${{ matrix.release }} -o jsonpath='{.items[0].metadata.name}' || true)
          if [ -n "$POD" ]; then
            echo "Fetching logs from pod $POD..."
            kubectl logs $POD -n ${{ matrix.namespace }} --tail=50
          fi
